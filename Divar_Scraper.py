# -*- coding: utf-8 -*-
"""
Divar Full Scraper - Ù†Ø³Ø®Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø´Ø¯Ù‡ Ø¨Ø§ AI
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾ÛŒÙ†Ú¯
- ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬
- Ù…Ø¯ÛŒØ±ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Ø®Ø·Ø§Ù‡Ø§
- Ø§Ø¶Ø§ÙÙ‡: Ø§ØªÙˆ-Ø³ÛŒÙˆ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ (checkpoint) Ùˆ resume Ø®ÙˆØ¯Ú©Ø§Ø±
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Docker
"""
from __future__ import annotations

import os
import re
import csv
import time
import json
import random
import traceback
import logging
from typing import List, Dict, Optional, Set, Any, Tuple
from collections import OrderedDict
from datetime import datetime

import pandas as pd
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.remote.remote_connection import LOGGER as SELENIUM_LOGGER

import socket  # ğŸ‘ˆ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ú†Ú© Ø§ÛŒÙ†ØªØ±Ù†Øª

# Ú©Ø§Ù‡Ø´ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Selenium
SELENIUM_LOGGER.setLevel(logging.WARNING)


def wait_for_internet(host="8.8.8.8", port=53, timeout=5, retry_delay=10):
    """ ØªØ§ ÙˆÙ‚ØªÛŒ Ø§ÛŒÙ†ØªØ±Ù†Øª ÙˆØµÙ„ Ø¨Ø´Ù‡ ØµØ¨Ø± Ù…ÛŒÚ©Ù†Ù‡ """
    while True:
        try:
            socket.setdefaulttimeout(timeout)
            socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((host, port))
            return
        except Exception:
            log("âŒ Ø§ÛŒÙ†ØªØ±Ù†Øª Ù‚Ø·Ø¹ Ø§Ø³ØªØŒ Ù…Ù†ØªØ¸Ø± Ø§ØªØµØ§Ù„...")
            time.sleep(retry_delay)


# ------------------------------------------------------------------
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
USE_WEBDRIVER_MANAGER = True
LOCAL_CHROMEDRIVER_PATH = ""

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ø§Ø±Ø¨Ø±
CITY_SLUG = "shiraz"
CATEGORY_NAME = "ÙØ±ÙˆØ´ Ù…Ø³Ú©ÙˆÙ†ÛŒ"
CATEGORY_URL = f"https://divar.ir/s/{CITY_SLUG}/buy-residential"

OUTPUT_XLSX = "divar_sales_ai.xlsx"
SEEN_LINKS_CSV = "seen_links_ai.csv"
SEEN_LINKS_JSON = "seen_links_ai.json"
AI_LEARNING_FILE = "ai_learning_data.json"
CHECKPOINT_FILE = "checkpoint_ai.json"  # ÙØ§ÛŒÙ„ checkpoint

# Ø±ÙØªØ§Ø± Ø§Ø³Ú©Ø±ÙˆÙ„/ØªØ£Ø®ÛŒØ±Ù‡Ø§ - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆØ±
IMPLICIT_WAIT = 1
PAGE_LOAD_SLEEP = (0.5, 1.0)
LIST_SCROLL_SLEEP = (0.4, 0.8)
SCROLL_MAX_ROUNDS = 350
SCROLL_PATIENCE = 7
SCROLL_EXTRA_AFTER_STABLE = 2
DETAIL_DWELL = (0.5, 1.0)
CLICK_VIEW_MORE_SLEEP = (1.0, 1.5)
BETWEEN_ADS_SLEEP = (0.3, 0.8)

# Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø³ØªÙˆÙ†ÛŒ (Ø¨Ø±Ú†Ø³Ø¨ Ù†Ù…Ø§ÛŒØ´ -> Ù†Ø§Ù… Ø³ØªÙˆÙ†)
FEATURES_MAP = {
    "Ø¢Ø³Ø§Ù†Ø³ÙˆØ±": "elevator",
    "Ù¾Ø§Ø±Ú©ÛŒÙ†Ú¯": "parking",
    "Ø§Ù†Ø¨Ø§Ø±ÛŒ": "storage_room",
    "Ø¨Ø§Ù„Ú©Ù†": "balcony",
    "Ø¬Ù†Ø³ Ú©Ù Ø³Ø±Ø§Ù…ÛŒÚ©": "floor_material_ceramic",
    "Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ": "iranian_wc",
    "Ø³Ø±Ù…Ø§ÛŒØ´ Ú©ÙˆÙ„Ø± Ø¢Ø¨ÛŒ": "cooling_evaporative",
    "Ú¯Ø±Ù…Ø§ÛŒØ´ Ø´ÙˆÙØ§Ú˜": "heating_radiator",
    "ØªØ§Ù”Ù…ÛŒÙ†â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù… Ù¾Ú©ÛŒØ¬": "hot_water_package",
}

# Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø¬Ø¯ÛŒØ¯ + Ø³ØªÙˆÙ† ØªØ§Ø±ÛŒØ® Ø§ÛŒØ¬Ø§Ø¯
BASE_COLUMNS = [
    "category", "Ù„ÛŒÙ†Ú©", "Ø¹Ù†ÙˆØ§Ù†", "ØªØ§Ø±ÛŒØ®", "Ù…Ú©Ø§Ù†",
    "Ù…ØªØ±Ø§Ú˜", "Ø³Ø§Ù„ Ø³Ø§Ø®Øª", "ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚", "ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ø­Ø¯ Ø¯Ø± Ø·Ø¨Ù‚Ù‡",
    "Ù†ÙˆØ¹ Ø³Ù†Ø¯", "ÙˆØ¶Ø¹ÛŒØª ÙˆØ§Ø­Ø¯", "Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†",
    "Ù‚ÛŒÙ…Øª Ú©Ù„", "Ù‚ÛŒÙ…Øª Ù‡Ø± Ù…ØªØ±", "Ø·Ø¨Ù‚Ù‡",
    "Ø¬Ù†Ø³ Ú©Ù", "Ù†ÙˆØ¹ Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ", "Ù†ÙˆØ¹ Ø³Ø±Ù…Ø§ÛŒØ´", "Ù†ÙˆØ¹ Ú¯Ø±Ù…Ø§ÛŒØ´", "ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù…",
    "ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø§Ù…Ú©Ø§Ù†Ø§Øª", "ØªÙˆØ¶ÛŒØ­Ø§Øª", "ØªØ§Ø±ÛŒØ® Ø§ÛŒØ¬Ø§Ø¯"  # Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
]
FINAL_COLUMNS = BASE_COLUMNS + list(FEATURES_MAP.values())


def get_current_timestamp() -> str:
    """Ø¯Ø±ÛŒØ§ÙØª ØªØ§Ø±ÛŒØ® Ùˆ Ø³Ø§Ø¹Øª ÙØ¹Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ† ØªØ§Ø±ÛŒØ® Ø§ÛŒØ¬Ø§Ø¯"""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def clean_numeric_fields(data: Dict[str, str]) -> Dict[str, str]:
    """
    Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¹Ø¯Ø¯
    """
    numeric_fields = [
        'Ù‚ÛŒÙ…Øª Ú©Ù„', 'Ù‚ÛŒÙ…Øª Ù‡Ø± Ù…ØªØ±', 'Ù…ØªØ±Ø§Ú˜', 'Ø³Ø§Ù„ Ø³Ø§Ø®Øª',
        'ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚', 'ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ø­Ø¯ Ø¯Ø± Ø·Ø¨Ù‚Ù‡', 'Ø·Ø¨Ù‚Ù‡'
    ]

    for field in numeric_fields:
        if field in data:
            value = data[field]

            # Ø§Ú¯Ø± Ù†Ø§Ù…Ø´Ø®Øµ Ø¨ÙˆØ¯ØŒ null Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡
            if value in ['Ù†Ø§Ù…Ø´Ø®Øµ', '']:
                data[field] = None
                continue

            # Ø­Ø°Ù Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¹Ø¯Ø¯ÛŒ
            cleaned_value = re.sub(r'[^\d]', '', str(value))

            # Ø§Ú¯Ø± Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ú†ÛŒØ²ÛŒ Ù†Ù…Ø§Ù†Ø¯ØŒ null Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡
            if not cleaned_value:
                data[field] = None
            else:
                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¹Ø¯Ø¯
                try:
                    data[field] = int(cleaned_value)
                except ValueError:
                    data[field] = None

    return data


def map_feature_columns(label_list: List[str]) -> Dict[str, str]:
    """
    ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
    """
    out = {}
    if not label_list:
        # Ø§Ú¯Ø± Ù„ÛŒØ³Øª Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ØŒ Ù‡Ù…Ù‡ Ø±Ùˆ Ù†Ø¯Ø§Ø±Ø¯ Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡
        for col in FEATURES_MAP.values():
            out[col] = "Ù†Ø¯Ø§Ø±Ø¯"
        return out

    s = set(label_list)
    for fa, col in FEATURES_MAP.items():
        # ÙÙ‚Ø· Ø§Ú¯Ø± Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø´ØªØŒ Ø¯Ø§Ø±Ø¯ Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡
        out[col] = "Ø¯Ø§Ø±Ø¯" if any(fa in x for x in s) else "Ù†Ø¯Ø§Ø±Ø¯"
    return out


def find_value_by_title(soup: BeautifulSoup, title_text: str) -> str:
    """
    Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù†ÙˆØ§Ù† Ø¯Ø± Ú©Ù„ ØµÙØ­Ù‡
    """
    try:
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ØªÙ…Ø§Ù… Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§
        all_elements = soup.find_all(["p", "div", "span"])

        for element in all_elements:
            text = element.get_text(strip=True)
            if title_text in text:
                # Ø³Ø¹ÛŒ Ú©Ù† Ù…Ù‚Ø¯Ø§Ø± Ø±Ùˆ Ø§Ø² Ø§Ù„Ù…Ø§Ù† Ø¨Ø¹Ø¯ÛŒ ÛŒØ§ Ù‡Ù… Ø³Ø·Ø­ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒ
                next_element = element.find_next()
                if next_element and next_element != element:
                    next_text = next_element.get_text(strip=True)
                    if next_text and title_text not in next_text:
                        return next_text

                # ÛŒØ§ Ø§Ø² parent Ùˆ siblings
                parent = element.parent
                if parent:
                    siblings = parent.find_all(["p", "div", "span"])
                    for sibling in siblings:
                        sibling_text = sibling.get_text(strip=True)
                        if sibling != element and sibling_text and title_text not in sibling_text:
                            return sibling_text

                return "Ù†Ø§Ù…Ø´Ø®Øµ"

        return "Ù†Ø§Ù…Ø´Ø®Øµ"

    except Exception:
        return "Ù†Ø§Ù…Ø´Ø®Øµ"


def extract_specific_details(soup: BeautifulSoup, data: Dict[str, str]) -> None:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®Ø§Øµ Ø§Ø² Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ú©Ù„Ø§Ø³ Ù…Ø´Ø®Øµ - Ù†Ø³Ø®Ù‡ Ø¯Ù‚ÛŒÙ‚
    """
    try:
        log("ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®Ø§Øµ...")

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ù‚ÛŒÙ‚ Ø§Ø² Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ kt-unexpandable-row
        rows = soup.find_all("div", class_=re.compile(r"kt-base-row|kt-unexpandable-row"))

        for row in rows:
            try:
                title_element = row.find("p", class_=re.compile(r"kt-base-row__title|kt-unexpandable-row__title"))
                value_element = row.find("p", class_=re.compile(r"kt-unexpandable-row__value|value"))

                if title_element and value_element:
                    title_text = title_element.get_text(strip=True)
                    value_text = value_element.get_text(strip=True)

                    if "ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ø­Ø¯ Ø¯Ø± Ø·Ø¨Ù‚Ù‡" in title_text:
                        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ
                        cleaned_value = re.sub(r'[^\d]', '', value_text)
                        data["ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ø­Ø¯ Ø¯Ø± Ø·Ø¨Ù‚Ù‡"] = int(cleaned_value) if cleaned_value else None

                    elif "Ù†ÙˆØ¹ Ø³Ù†Ø¯" in title_text or "Ø³Ù†Ø¯" == title_text.strip():
                        data["Ù†ÙˆØ¹ Ø³Ù†Ø¯"] = value_text if value_text != "Ù†Ø§Ù…Ø´Ø®Øµ" else None

                    elif "ÙˆØ¶Ø¹ÛŒØª ÙˆØ§Ø­Ø¯" in title_text:
                        data["ÙˆØ¶Ø¹ÛŒØª ÙˆØ§Ø­Ø¯"] = value_text if value_text != "Ù†Ø§Ù…Ø´Ø®Øµ" else None

                    elif "Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†" in title_text or "Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†" == title_text.strip():
                        data["Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†"] = value_text if value_text != "Ù†Ø§Ù…Ø´Ø®Øµ" else None

                    elif "Ù‚ÛŒÙ…Øª Ú©Ù„" in title_text:
                        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ
                        cleaned_value = re.sub(r'[^\d]', '', value_text)
                        data["Ù‚ÛŒÙ…Øª Ú©Ù„"] = int(cleaned_value) if cleaned_value else None

                    elif "Ù‚ÛŒÙ…Øª Ù‡Ø± Ù…ØªØ±" in title_text:
                        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ
                        cleaned_value = re.sub(r'[^\d]', '', value_text)
                        data["Ù‚ÛŒÙ…Øª Ù‡Ø± Ù…ØªØ±"] = int(cleaned_value) if cleaned_value else None

                    elif "Ø·Ø¨Ù‚Ù‡" in title_text:
                        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¹Ø¯Ø¯ÛŒ
                        cleaned_value = re.sub(r'[^\d]', '', value_text)
                        data["Ø·Ø¨Ù‚Ù‡"] = int(cleaned_value) if cleaned_value else None

            except:
                continue

        if data.get("Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†") in [None, "Ù†Ø§Ù…Ø´Ø®Øµ", ""]:
            try:
                direction_title = soup.find("p", class_="kt-base-row__title", string="Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†")
                if direction_title:
                    direction_value = direction_title.find_next_sibling("p", class_="kt-unexpandable-row__value")
                    if direction_value:
                        data["Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†"] = direction_value.get_text(strip=True)
                        log(f"âœ… Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù† Ù…Ø³ØªÙ‚ÛŒÙ… Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {data['Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†']}")
            except Exception as e:
                log(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†: {e}")

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ±Ø§Ú˜ØŒ Ø³Ø§Ù„ Ø³Ø§Ø®ØªØŒ ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚
        try:
            info_rows = soup.select("tr.kt-group-row__data-row")
            for row in info_rows:
                cells = row.select("td.kt-group-row-item--info-row, td.kt-group-row-item.kt-group-row-item__value")
                if cells:
                    vals = [c.get_text(" ", strip=True) for c in cells]
                    if len(vals) >= 3:
                        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ
                        meterage_clean = re.sub(r'[^\d]', '', vals[0])
                        year_clean = re.sub(r'[^\d]', '', vals[1])
                        rooms_clean = re.sub(r'[^\d]', '', vals[2])

                        data["Ù…ØªØ±Ø§Ú˜"] = int(meterage_clean) if meterage_clean else None
                        data["Ø³Ø§Ù„ Ø³Ø§Ø®Øª"] = int(year_clean) if year_clean else None
                        data["ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚"] = int(rooms_clean) if rooms_clean else None
                        break
        except:
            pass

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø² Ø¨Ø®Ø´ kt-feature-row
        feature_elements = soup.find_all("div", class_=re.compile(r"kt-feature-row"))
        all_features = []

        for feature in feature_elements:
            try:
                title_element = feature.find("p", class_=re.compile(r"kt-feature-row__title"))
                if title_element:
                    feature_text = title_element.get_text(strip=True)
                    all_features.append(feature_text)

                    # Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ù…Ù‚Ø¯Ø§Ø± Ø±Ùˆ Ø¯Ø± ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡
                    if any(x in feature_text for x in ["Ø¬Ù†Ø³ Ú©Ù", "Ú©Ù", "Ø³Ø±Ø§Ù…ÛŒÚ©", "Ù…ÙˆØ²Ø§ÛŒÛŒÚ©", "Ø³Ù†Ú¯"]):
                        data["Ø¬Ù†Ø³ Ú©Ù"] = feature_text if feature_text != "Ù†Ø§Ù…Ø´Ø®Øµ" else None
                    elif any(x in feature_text for x in ["Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ", "Ø¯Ø³ØªØ´ÙˆÛŒÛŒ", "ØªÙˆØ§Ù„Øª", "Ø­Ù…Ø§Ù…"]):
                        data["Ù†ÙˆØ¹ Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ"] = feature_text if feature_text != "Ù†Ø§Ù…Ø´Ø®Øµ" else None
                    elif any(x in feature_text for x in ["Ø³Ø±Ù…Ø§ÛŒØ´", "Ú©ÙˆÙ„Ø±", "ØªÙ‡ÙˆÛŒÙ‡", "Ù‡ÙˆØ§Ø³Ø§Ø²"]):
                        data["Ù†ÙˆØ¹ Ø³Ø±Ù…Ø§ÛŒØ´"] = feature_text if feature_text != "Ù†Ø§Ù…Ø´Ø®Øµ" else None
                    elif any(x in feature_text for x in ["Ú¯Ø±Ù…Ø§ÛŒØ´", "Ø´ÙˆÙØ§Ú˜", "Ø¨Ø®Ø§Ø±ÛŒ", "Ø±Ø§Ø¯ÛŒØ§ØªÙˆØ±"]):
                        data["Ù†ÙˆØ¹ Ú¯Ø±Ù…Ø§ÛŒØ´"] = feature_text if feature_text != "Ù†Ø§Ù…Ø´Ø®Øµ" else None
                    elif any(x in feature_text for x in ["Ø¢Ø¨ Ú¯Ø±Ù…", "Ù¾Ú©ÛŒØ¬", "Ù…Ù†Ø¨Ø¹", "Ù…ÙˆØªÙˆØ±Ø®Ø§Ù†Ù‡"]):
                        data["ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù…"] = feature_text if feature_text != "Ù†Ø§Ù…Ø´Ø®Øµ" else None

            except:
                continue

        # Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±Ùˆ Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†
        for feature in all_features:
            if "Ø¢Ø³Ø§Ù†Ø³ÙˆØ±" in feature:
                data["elevator"] = "Ø¯Ø§Ø±Ø¯"
            if "Ù¾Ø§Ø±Ú©ÛŒÙ†Ú¯" in feature:
                data["parking"] = "Ø¯Ø§Ø±Ø¯"
            if "Ø§Ù†Ø¨Ø§Ø±ÛŒ" in feature:
                data["storage_room"] = "Ø¯Ø§Ø±Ø¯"
            if "Ø¨Ø§Ù„Ú©Ù†" in feature:
                data["balcony"] = "Ø¯Ø§Ø±Ø¯"
            if "Ø¬Ù†Ø³ Ú©Ù Ø³Ø±Ø§Ù…ÛŒÚ©" in feature:
                data["floor_material_ceramic"] = "Ø¯Ø§Ø±Ø¯"
                data["Ø¬Ù†Ø³ Ú©Ù"] = "Ø³Ø±Ø§Ù…ÛŒÚ©"
            if "Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ" in feature:
                data["iranian_wc"] = "Ø¯Ø§Ø±Ø¯"
                data["Ù†ÙˆØ¹ Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ"] = "Ø§ÛŒØ±Ø§Ù†ÛŒ"
            if "Ø³Ø±Ù…Ø§ÛŒØ´ Ú©ÙˆÙ„Ø± Ø¢Ø¨ÛŒ" in feature:
                data["cooling_evaporative"] = "Ø¯Ø§Ø±Ø¯"
                data["Ù†ÙˆØ¹ Ø³Ø±Ù…Ø§ÛŒØ´"] = "Ú©ÙˆÙ„Ø± Ø¢Ø¨ÛŒ"
            if "Ú¯Ø±Ù…Ø§ÛŒØ´ Ø´ÙˆÙØ§Ú˜" in feature:
                data["heating_radiator"] = "Ø¯Ø§Ø±Ø¯"
                data["Ù†ÙˆØ¹ Ú¯Ø±Ù…Ø§ÛŒØ´"] = "Ø´ÙˆÙØ§Ú˜"
            if "ØªØ£Ù…ÛŒÙ†â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù… Ù¾Ú©ÛŒØ¬" in feature:
                data["hot_water_package"] = "Ø¯Ø§Ø±Ø¯"
                data["ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù…"] = "Ù¾Ú©ÛŒØ¬"

        # Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù‡Ù†ÙˆØ² Ù¾Ø± Ù†Ø´Ø¯Ù†ØŒ Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡
        text_fields_defaults = {
            "Ø¬Ù†Ø³ Ú©Ù": None,
            "Ù†ÙˆØ¹ Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ": None,
            "Ù†ÙˆØ¹ Ø³Ø±Ù…Ø§ÛŒØ´": None,
            "Ù†ÙˆØ¹ Ú¯Ø±Ù…Ø§ÛŒØ´": None,
            "ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù…": None
        }

        for field, default_val in text_fields_defaults.items():
            if data.get(field) in [None, "Ù†Ø§Ù…Ø´Ø®Øµ", ""]:
                data[field] = default_val

        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø§Ù…Ú©Ø§Ù†Ø§Øª
        data["ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø§Ù…Ú©Ø§Ù†Ø§Øª"] = "ØŒ ".join(all_features) if all_features else None

        # Ø¨Ø±Ø§ÛŒ Ø§Ù…Ú©Ø§Ù†Ø§ØªÛŒ Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ù†Ø¯Ø§Ø±Ù†ØŒ Ù†Ø¯Ø§Ø±Ø¯ Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡
        for col in FEATURES_MAP.values():
            if col not in data:
                data[col] = "Ù†Ø¯Ø§Ø±Ø¯"

        log(f"âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {len(all_features)} ÙˆÛŒÚ˜Ú¯ÛŒ")

    except Exception as e:
        log(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®Ø§Øµ: {e}")
        # Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ ÙÛŒÙ„Ø¯Ù‡Ø§
        default_fields = {
            "ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ø­Ø¯ Ø¯Ø± Ø·Ø¨Ù‚Ù‡": None,
            "Ù†ÙˆØ¹ Ø³Ù†Ø¯": None,
            "ÙˆØ¶Ø¹ÛŒØª ÙˆØ§Ø­Ø¯": None,
            "Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†": None,
            "Ù‚ÛŒÙ…Øª Ú©Ù„": None,
            "Ù‚ÛŒÙ…Øª Ù‡Ø± Ù…ØªØ±": None,
            "Ø·Ø¨Ù‚Ù‡": None,
            "Ù…ØªØ±Ø§Ú˜": None,
            "Ø³Ø§Ù„ Ø³Ø§Ø®Øª": None,
            "ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚": None,
            "Ø¬Ù†Ø³ Ú©Ù": None,
            "Ù†ÙˆØ¹ Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ": None,
            "Ù†ÙˆØ¹ Ø³Ø±Ù…Ø§ÛŒØ´": None,
            "Ù†ÙˆØ¹ Ú¯Ø±Ù…Ø§ÛŒØ´": None,
            "ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù…": None,
            "ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø§Ù…Ú©Ø§Ù†Ø§Øª": None
        }
        data.update(default_fields)

        for col in FEATURES_MAP.values():
            data[col] = "Ù†Ø¯Ø§Ø±Ø¯"


def find_in_text(soup: BeautifulSoup, primary_term: str, secondary_term: str) -> str:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ÛŒÚ© Ø¹Ø¨Ø§Ø±Øª Ø¯Ø± Ù…ØªÙ† ØµÙØ­Ù‡
    """
    try:
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ØªÙ…Ø§Ù… Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§
        for element in soup.find_all(["p", "div", "span"]):
            text = element.get_text(strip=True)
            if primary_term in text or secondary_term in text:
                # Ø³Ø¹ÛŒ Ú©Ù†ÛŒÙ… Ù…Ù‚Ø¯Ø§Ø± Ø±Ùˆ Ø§Ø² Ø§Ù„Ù…Ø§Ù† Ù…Ø¬Ø§ÙˆØ± Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒÙ…
                parent = element.parent
                if parent:
                    siblings = parent.find_all(["p", "div", "span"])
                    for sibling in siblings:
                        if sibling != element and sibling.get_text(strip=True):
                            return sibling.get_text(strip=True)

                # ÛŒØ§ Ø§Ø² Ù…ØªÙ† Ø®ÙˆØ¯ Ø§Ù„Ù…Ø§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
                return text

        return "Ù†Ø§Ù…Ø´Ø®Øµ"
    except:
        return "Ù†Ø§Ù…Ø´Ø®Øµ"


# ----------------------------- Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ -----------------------------
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('divar_scraper_ai.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


def log(msg: str, level: str = "INFO") -> None:
    log_level = getattr(logging, level.upper())
    logging.log(log_level, msg)


def human_sleep(a: float, b: float) -> None:
    time.sleep(random.uniform(a, b))


def ensure_dir_for_file(path: str) -> None:
    d = os.path.dirname(os.path.abspath(path))
    if d and not os.path.exists(d):
        os.makedirs(d, exist_ok=True)


def read_seen_links_csv(path: str) -> Set[str]:
    if not os.path.exists(path):
        return set()
    s = set()
    with open(path, "r", encoding="utf-8") as f:
        rdr = csv.reader(f)
        for row in rdr:
            if row:
                s.add(row[0].strip())
    return s


def append_seen_links_csv(path: str, links: List[str]) -> None:
    if not links:
        return
    ensure_dir_for_file(path)
    with open(path, "a", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        for lk in links:
            w.writerow([lk])


def read_seen_links_json(path: str) -> Set[str]:
    if not os.path.exists(path):
        return set()
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            return set(data) if isinstance(data, list) else set()
    except Exception:
        return set()


def write_seen_links_json(path: str, links: Set[str]) -> None:
    ensure_dir_for_file(path)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(list(links), f, ensure_ascii=False, indent=2)


def load_existing_links_from_excel(path: str) -> Set[str]:
    if not os.path.exists(path):
        return set()
    try:
        df = pd.read_excel(path)
        if "Ù„ÛŒÙ†Ú©" in df.columns:
            return set(df["Ù„ÛŒÙ†Ú©"].astype(str).str.strip().tolist())
    except Exception:
        pass
    return set()


# ----------------------------- checkpoint helpers -----------------------------
def atomic_write_json(path: str, data: Any) -> None:
    """Ù†ÙˆØ´ØªÙ† Ø§ÛŒÙ…Ù† JSON (atomic)"""
    ensure_dir_for_file(path)
    tmp = f"{path}.tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    try:
        os.replace(tmp, path)
    except Exception:
        try:
            os.remove(path)
            os.replace(tmp, path)
        except Exception:
            pass


def load_checkpoint(path: str) -> Optional[Dict[str, Any]]:
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data
    except Exception as e:
        log(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ checkpoint: {e}")
        return None


def save_checkpoint(path: str, state: Dict[str, Any]) -> None:
    """Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ (Ù¾Ø³ Ø§Ø² Ù‡Ø± Ø¢Ú¯Ù‡ÛŒ)"""
    try:
        atomic_write_json(path, state)
        log(f"ğŸ’¾ checkpoint Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {path} (processed: {len(state.get('processed_links', []))})")
    except Exception as e:
        log(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ checkpoint: {e}")


def clear_checkpoint(path: str) -> None:
    try:
        if os.path.exists(path):
            os.remove(path)
            log("ğŸ§¹ checkpoint Ù¾Ø§Ú© Ø´Ø¯.")
    except Exception as e:
        log(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©â€ŒÚ©Ø±Ø¯Ù† checkpoint: {e}")


# ----------------------------- Ú©Ù„Ø§Ø³ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² AI -----------------------------
class AIScrapingOptimizer:
    def __init__(self):
        self.scraping_patterns = []
        self.error_patterns = []
        self.success_rates = {}
        self.learning_data = self._load_learning_data()

    def _load_learning_data(self) -> List[Dict]:
        if os.path.exists(AI_LEARNING_FILE):
            try:
                with open(AI_LEARNING_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return []
        return []

    def _save_learning_data(self):
        ensure_dir_for_file(AI_LEARNING_FILE)
        with open(AI_LEARNING_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.learning_data, f, ensure_ascii=False, indent=2)

    def analyze_page_structure(self, driver, page_type="list") -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ø§Ø®ØªØ§Ø± ØµÙØ­Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ¹ÛŒÛŒÙ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø§Ø³Ú©Ø±Ù¾"""
        try:
            key_elements = self._identify_key_elements(driver, page_type)

            # Ø¯ÛŒØ¨Ø§Ú¯: Ù†Ù…Ø§ÛŒØ´ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡
            log(f"ØªØ¬Ø²ÛŒÙ‡ ØµÙØ­Ù‡ {page_type}: {key_elements}")

            optimal_strategy = self._determine_optimal_strategy(key_elements, page_type)

            return {
                "strategy": optimal_strategy,
                "key_elements": key_elements,
                "confidence_score": self._calculate_confidence(key_elements)
            }

        except Exception as e:
            log(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø®ØªØ§Ø± ØµÙØ­Ù‡: {e}")
            return self._get_fallback_strategy(page_type)

    def _identify_key_elements(self, driver, page_type) -> Dict[str, Any]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… ØµÙØ­Ù‡"""
        elements = {}

        try:
            if page_type == "list":
                # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ø¢Ú¯Ù‡ÛŒ
                ad_candidates = driver.find_elements(By.XPATH,
                                                     "//article | //div[contains(@class, 'card')] | //div[contains(@class, 'post')]")
                elements['ad_containers'] = len(ad_candidates)

                # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§ÛŒ pagination
                pagination_elements = driver.find_elements(By.XPATH,
                                                           "//a[contains(@href, 'page')] | //button[contains(text(), 'Ø¨Ø¹Ø¯ÛŒ')]")
                elements['has_pagination'] = len(pagination_elements) > 0

                # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ø³Ú©Ø±ÙˆÙ„ infinit
                scroll_height = driver.execute_script("return document.body.scrollHeight")
                viewport_height = driver.execute_script("return window.innerHeight")
                elements['is_infinite_scroll'] = scroll_height > viewport_height * 3

            elif page_type == "detail":
                # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ
                info_sections = driver.find_elements(By.XPATH, "//div[contains(@class, 'info')] | //table | //dl")
                elements['info_sections'] = len(info_sections)

                # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¯Ú©Ù…Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¨ÛŒØ´ØªØ±
                show_more_buttons = driver.find_elements(By.XPATH,
                                                         "//button[contains(text(), 'Ù†Ù…Ø§ÛŒØ´')] | //a[contains(text(), 'Ù†Ù…Ø§ÛŒØ´')]")
                elements['has_show_more'] = len(show_more_buttons) > 0

        except Exception as e:
            log(f"Ø®Ø·Ø§ Ø¯Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§: {e}")

        return elements

    def _determine_optimal_strategy(self, elements, page_type) -> Dict[str, Any]:
        """ØªØ¹ÛŒÛŒÙ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡"""
        if page_type == "list":
            if elements.get('is_infinite_scroll', False):
                return {
                    "type": "infinite_scroll",
                    "scroll_increment": 800,
                    "scroll_delay": (0.8, 1.2),
                    "max_attempts": 15
                }
            elif elements.get('has_pagination', False):
                return {
                    "type": "pagination",
                    "page_load_delay": (1.5, 2.0)
                }
            else:
                return {
                    "type": "standard_scroll",
                    "scroll_increment": 600,
                    "scroll_delay": (1.0, 1.5)
                }

        elif page_type == "detail":
            if elements.get('has_show_more', False):
                return {
                    "type": "click_show_more",
                    "wait_after_click": (2.0, 3.0)
                }
            else:
                return {
                    "type": "direct_extraction",
                    "extraction_delay": (1.0, 1.8)
                }

    def _get_fallback_strategy(self, page_type) -> Dict[str, Any]:
        """Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ fallback Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§"""
        if page_type == "list":
            return {
                "type": "standard_scroll",
                "scroll_increment": 600,
                "scroll_delay": (1.0, 1.5)
            }
        else:
            return {
                "type": "direct_extraction",
                "extraction_delay": (1.0, 1.8)
            }

    def _calculate_confidence(self, elements) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ²Ø§Ù† Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ­Ù„ÛŒÙ„"""
        if not elements:
            return 0.5

        confidence = 0.5
        if elements.get('ad_containers', 0) > 0:
            confidence += 0.2
        if elements.get('info_sections', 0) > 0:
            confidence += 0.2

        return min(confidence, 1.0)

    def optimize_extraction_selectors(self, soup, current_data) -> Dict[str, str]:
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ù„Ú©ØªÙˆØ±Ù‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡"""
        optimized_selectors = {}

        for field in ['Ù…ØªØ±Ø§Ú˜', 'Ù‚ÛŒÙ…Øª Ú©Ù„', 'ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚', 'Ø³Ø§Ù„ Ø³Ø§Ø®Øª']:
            best_selector = self._find_best_selector_for_field(soup, field, current_data.get(field, ""))
            if best_selector:
                optimized_selectors[field] = best_selector

        return optimized_selectors

    def _find_best_selector_for_field(self, soup, field_name, current_value) -> Optional[str]:
        """ÛŒØ§ÙØªÙ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø³Ù„Ú©ØªÙˆØ± Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙÛŒÙ„Ø¯"""
        patterns = [
            f"//*[contains(text(), '{field_name}')]/following-sibling::*",
            f"//*[contains(@class, '{field_name.lower()}')]",
            f"//*[contains(text(), '{field_name.split()[0]}')]",
        ]

        for pattern in patterns:
            try:
                elements = soup.select(pattern) if pattern.startswith('.') else soup.find_all(pattern)
                if elements and any(self._is_valid_value(elem.get_text(), field_name) for elem in elements):
                    return pattern
            except:
                continue

        return None

    def _is_valid_value(self, value, field_name) -> bool:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡"""
        value = value.strip()
        if not value or value == "Ù†Ø§Ù…Ø´Ø®Øµ":
            return False

        if field_name == "Ù…ØªØ±Ø§Ú˜" and "Ù…ØªØ±" in value:
            return True
        if field_name == "Ù‚ÛŒÙ…Øª Ú©Ù„" and ("ØªÙˆÙ…Ø§Ù†" in value or "Ø±ÛŒØ§Ù„" in value):
            return True
        if field_name == "ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚" and any(char.isdigit() for char in value):
            return True

        return len(value) > 1

    def learn_from_results(self, url, strategy_used, success_rate, extracted_data) -> None:
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡"""
        learning_entry = {
            "url": url,
            "strategy": strategy_used,
            "success_rate": success_rate,
            "timestamp": time.time(),
            "data_quality": self._calculate_data_quality(extracted_data)
        }

        self.learning_data.append(learning_entry)
        self._save_learning_data()

        if success_rate > 0.8:
            self.scraping_patterns.append(strategy_used)

        self.scraping_patterns = [pattern for pattern in self.scraping_patterns
                                  if self._get_pattern_success_rate(pattern) > 0.6]

    def _calculate_data_quality(self, data) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡"""
        if not data:
            return 0.0

        required_fields = ['Ø¹Ù†ÙˆØ§Ù†', 'Ù…ØªØ±Ø§Ú˜', 'Ù‚ÛŒÙ…Øª Ú©Ù„']
        quality_score = 0.0

        for field in required_fields:
            if field in data and data[field] not in [None, "", "Ù†Ø§Ù…Ø´Ø®Øµ"]:
                quality_score += 0.3

        return min(quality_score, 1.0)

    def get_recommended_strategy(self, page_type) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‚Ø¨Ù„ÛŒ"""
        if not self.scraping_patterns:
            return self._get_fallback_strategy(page_type)

        best_pattern = max(self.scraping_patterns,
                           key=lambda x: self._get_pattern_success_rate(x))

        return best_pattern

    def _get_pattern_success_rate(self, pattern) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª ÛŒÚ© Ø§Ù„Ú¯Ùˆ"""
        relevant_entries = [entry for entry in self.learning_data
                            if entry['strategy']['type'] == pattern['type']]

        if not relevant_entries:
            return 0.5

        return sum(entry['success_rate'] for entry in relevant_entries) / len(relevant_entries)


# ----------------------------- Ø¯Ø±Ø§ÛŒÙˆØ± (Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Docker) -----------------------------
def build_driver(headless: bool = True) -> webdriver.Chrome:
    """
    Ø³Ø§Ø®Øª Ùˆ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¯Ø±Ø§ÛŒÙˆØ± Chrome - Ù†Ø³Ø®Ù‡ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Docker
    """
    import os
    import tempfile
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options

    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø³ÛŒØ± cache Ø§Ù…Ù† Ø¨Ø§ Ø¯Ø³ØªØ±Ø³ÛŒ Ú©Ø§Ù…Ù„
    cache_dir = "/tmp/wdm_cache"
    os.makedirs(cache_dir, exist_ok=True)
    os.chmod(cache_dir, 0o777)

    # ØªÙ†Ø¸ÛŒÙ… Ù…Ø­ÛŒØ· Ø¨Ø±Ø§ÛŒ webdriver-manager
    os.environ['WDM_LOG_LEVEL'] = '0'
    os.environ['WDM_LOCAL'] = '1'
    os.environ['WDM_CACHE_PATH'] = cache_dir

    opts = Options()
    opts.page_load_strategy = 'eager'  # Ù„ÙˆØ¯ Ø³Ø±ÛŒØ¹ ØµÙØ­Ù‡

    # ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ± Chrome - ÙÙ‚Ø· Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
    chrome_path = "/usr/bin/google-chrome"
    if os.path.exists(chrome_path):
        opts.binary_location = chrome_path
        log(f"âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Chrome: {chrome_path}")
    else:
        log("âš ï¸ Chrome ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² chromedriver Ø¯Ø§Ø®Ù„ÛŒ")

    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø¬Ø¨Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Docker
    opts.add_argument("--blink-settings=imagesEnabled=false")  # ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† ØªØµØ§ÙˆÛŒØ±
    opts.add_argument("--disable-http2")
    opts.add_argument("--disable-quic")
    opts.add_argument("--disable-background-timer-throttling")
    opts.add_argument("--disable-backgrounding-occluded-windows")
    opts.add_argument("--disable-renderer-backgrounding")
    opts.add_argument("--disable-ipc-flooding-protection")
    opts.add_argument("--disable-client-side-phishing-detection")
    opts.add_argument("--disable-component-extensions-with-background-pages")
    opts.add_argument("--disable-default-apps")
    opts.add_argument("--disable-plugins")
    opts.add_argument("--disable-popup-blocking")
    opts.add_argument("--disable-prompt-on-repost")
    opts.add_argument("--disable-sync")
    opts.add_argument("--safebrowsing-disable-auto-update")
    opts.add_argument("--metrics-recording-only")
    opts.add_argument("--no-first-run")
    opts.add_argument("--no-default-browser-check")
    opts.add_argument("--media-cache-size=1")
    opts.add_argument("--disk-cache-size=1")
    opts.add_argument("--aggressive-cache-discard")

    if headless:
        opts.add_argument("--headless=new")

    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª user-agent Ùˆ Ø²Ø¨Ø§Ù†
    ua = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    opts.add_argument(f"--user-agent={ua}")
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    opts.add_argument("--window-size=1920,1080")
    opts.add_argument("--lang=fa-IR")

    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª experimental
    opts.add_experimental_option("prefs", {
        "profile.default_content_setting_values.notifications": 2,
        "profile.managed_default_content_settings.images": 2,  # ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† ØªØµØ§ÙˆÛŒØ±
        "profile.default_content_settings.popups": 0,
        "profile.default_content_settings.geolocation": 2,
        "profile.default_content_settings.media_stream": 2,  # ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† ÙˆÛŒØ¯ÛŒÙˆ/ØµØ¯Ø§
        "profile.default_content_settings.cookies": 2,
        "profile.default_content_settings.plugins": 2,
        "profile.default_content_settings.mixed_script": 2,
        "profile.default_content_settings.media_stream": 2,
    })

    max_retries = 3
    for attempt in range(max_retries):
        try:
            log(f"ğŸ”„ ØªÙ„Ø§Ø´ {attempt + 1}/{max_retries} Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯Ø±Ø§ÛŒÙˆØ±...")

            # Ø±ÙˆØ´ 1: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² webdriver-manager Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§
            try:
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager(cache_path=cache_dir).install())
            except Exception as e:
                log(f"âš ï¸ webdriver-manager Ø®Ø·Ø§ Ø®ÙˆØ±Ø¯: {e}")
                # Ø±ÙˆØ´ 2: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² chromedriver Ø§Ø² Ø³ÛŒØ³ØªÙ…
                service = Service("/usr/local/bin/chromedriver")

            driver = webdriver.Chrome(service=service, options=opts)

            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª timeout
            driver.set_page_load_timeout(30)
            driver.set_script_timeout(20)
            driver.implicitly_wait(10)

            # Ù…Ø®ÙÛŒ Ú©Ø±Ø¯Ù† automation
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            # ØªØ³Øª Ø³Ù„Ø§Ù…Øª Ø¯Ø±Ø§ÛŒÙˆØ±
            driver.get("about:blank")
            current_url = driver.current_url
            if "about:blank" in current_url:
                log("âœ… Ø¯Ø±Ø§ÛŒÙˆØ± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ùˆ ØªØ³Øª Ø´Ø¯")
                return driver
            else:
                raise Exception("ØªØ³Øª Ø³Ù„Ø§Ù…Øª Ø¯Ø±Ø§ÛŒÙˆØ± Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯")

        except Exception as e:
            log(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙ„Ø§Ø´ {attempt + 1}: {str(e)}")

            if attempt == max_retries - 1:
                log("ğŸ”¥ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±Ø§Ù‡Ú©Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ...")
                return _ultimate_fallback_driver(headless)

            import time
            time.sleep(2)


def _ultimate_fallback_driver(headless: bool = True) -> webdriver.Chrome:
    """
    Ø±Ø§Ù‡Ú©Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ Ø´Ú©Ø³Øª Ù…ÛŒâ€ŒØ®ÙˆØ±Ù†Ø¯
    """
    try:
        log("ğŸš¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±Ø§Ù‡Ú©Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ (ØªÙ†Ø¸ÛŒÙ…Ø§ Ù…ÛŒÙ†ÛŒÙ…Ø§Ù„)...")

        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.chrome.service import Service

        opts = Options()
        opts.add_argument("--no-sandbox")
        opts.add_argument("--disable-dev-shm-usage")

        if headless:
            opts.add_argument("--headless=new")

        # Ø³Ø¹ÛŒ Ú©Ù† chromedriver Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ù¾ÛŒØ¯Ø§ Ú©Ù†
        possible_paths = [
            "/usr/local/bin/chromedriver",
            "/usr/bin/chromedriver",
            "/app/chromedriver",
            "chromedriver"  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² PATH
        ]

        for path in possible_paths:
            try:
                service = Service(executable_path=path)
                driver = webdriver.Chrome(service=service, options=opts)
                log(f"âœ… Ø¯Ø±Ø§ÛŒÙˆØ± Ø¨Ø§ Ù…Ø³ÛŒØ± {path} Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
                return driver
            except:
                continue

        # Ø¢Ø®Ø±ÛŒÙ† ØªÙ„Ø§Ø´: Ø¨Ø¯ÙˆÙ† service
        driver = webdriver.Chrome(options=opts)
        log("âœ… Ø¯Ø±Ø§ÛŒÙˆØ± Ø¨Ø¯ÙˆÙ† service Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
        return driver

    except Exception as e:
        log(f"ğŸ’¥ Ø®Ø·Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯Ø±Ø§ÛŒÙˆØ±: {e}")
        raise Exception(f"Ø§Ù…Ú©Ø§Ù† Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯Ø±Ø§ÛŒÙˆØ± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {e}")


def check_system_dependencies():
    """
    Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¬Ø±Ø§ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
    """
    import os
    import subprocess
    import shutil

    log("ğŸ” Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…...")

    # Ø¨Ø±Ø±Ø³ÛŒ Python Ùˆ pip
    try:
        python_version = subprocess.run(["python3", "--version"], capture_output=True, text=True)
        log(f"âœ… Python: {python_version.stdout.strip()}")
    except Exception as e:
        log(f"âŒ Python Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯: {e}")

    # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±Ù‡Ø§ Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª
    browsers = [
        ("google-chrome", "/usr/bin/google-chrome"),
        ("chromium-browser", "/usr/bin/chromium-browser"),
        ("chromium", "/usr/bin/chromium")
    ]

    available_browsers = []
    for name, path in browsers:
        if os.path.exists(path):
            available_browsers.append((name, path))
            try:
                version = subprocess.run([path, "--version"], capture_output=True, text=True, timeout=5)
                log(f"âœ… {name}: {version.stdout.strip()}")
            except subprocess.TimeoutExpired:
                log(f"âš ï¸ {name}: timeout Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø³Ø®Ù‡")
            except Exception as e:
                log(f"âš ï¸ {name}: Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø³Ø®Ù‡ - {e}")
        else:
            log(f"âŒ {name}: ÛŒØ§ÙØª Ù†Ø´Ø¯")

    # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
    tools = ["unzip", "curl", "wget"]
    for tool in tools:
        if shutil.which(tool):
            log(f"âœ… {tool}: ÛŒØ§ÙØª Ø´Ø¯")
        else:
            log(f"âš ï¸ {tool}: ÛŒØ§ÙØª Ù†Ø´Ø¯")

    # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ
    test_dirs = ["/tmp", "/app", "/home"]
    for test_dir in test_dirs:
        if os.path.exists(test_dir):
            try:
                test_file = os.path.join(test_dir, "test_write")
                with open(test_file, "w") as f:
                    f.write("test")
                os.remove(test_file)
                log(f"âœ… Ø¯Ø³ØªØ±Ø³ÛŒ Ù†ÙˆØ´ØªÙ† Ø¯Ø± {test_dir}: Ù…Ø¬Ø§Ø²")
            except Exception as e:
                log(f"âŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ù†ÙˆØ´ØªÙ† Ø¯Ø± {test_dir}: Ù…Ù…Ù†ÙˆØ¹ - {e}")

    success = len(available_browsers) > 0
    if success:
        log("âœ… Ø³ÛŒØ³ØªÙ… Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª")
    else:
        log("âŒ Ù‡ÛŒÚ† Ù…Ø±ÙˆØ±Ú¯Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯! Ø§Ø¬Ø±Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø§ Ù…Ø´Ú©Ù„ Ù…ÙˆØ§Ø¬Ù‡ Ø´ÙˆØ¯")

    return success


def test_driver_connection():
    """
    ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¯Ø±Ø§ÛŒÙˆØ± Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ Ø¨Ù‡ØªØ±
    """
    try:
        log("ğŸ§ª ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¯Ø±Ø§ÛŒÙˆØ±...")

        # ØªØ³Øª Ø³Ø±ÛŒØ¹ Ø¨Ø¯ÙˆÙ† Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø¶Ø§ÙÛŒ
        driver = build_driver(headless=True)

        # ØªØ³Øª Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡
        driver.get("https://www.google.com")
        title = driver.title
        log(f"âœ… ØªØ³Øª Ø§ØªØµØ§Ù„ Ù…ÙˆÙÙ‚: {title}")

        driver.quit()
        return True

    except Exception as e:
        log(f"âŒ ØªØ³Øª Ø§ØªØµØ§Ù„ Ù†Ø§Ù…ÙˆÙÙ‚: {e}")
        return False


# ----------------------------- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ (Ù‡ÙˆØ´Ù…Ù†Ø¯) -----------------------------
def close_map_if_exists(driver: webdriver.Chrome) -> None:
    """Ø¨Ø³ØªÙ† Ù†Ù‚Ø´Ù‡ Ø´Ù†Ø§ÙˆØ± (FAB)"""
    try:
        candidates = driver.find_elements(By.CSS_SELECTOR, "div.kt-fab-button, div[class*='kt-fab-button']")
        candidates += driver.find_elements(By.XPATH, "//div[contains(@class,'kt-fab-button')]")
        for el in candidates:
            try:
                ActionChains(driver).move_to_element(el).pause(0.05).click(el).perform()
                log("Ù†Ù‚Ø´Ù‡ Ø¨Ø³ØªÙ‡ Ø´Ø¯.")
                human_sleep(1, 1.5)
                return
            except Exception:
                continue
    except Exception:
        pass


def get_ad_links_ai(category_url: str, category_name: str, ai_optimizer: AIScrapingOptimizer) -> List[str]:
    """
    Ø§Ø³Ú©Ø±ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ­Ù„ÛŒÙ„ AI Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
    """
    driver = build_driver(headless=True)  # headless=True Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆØ±
    try:
        log(f"ÙˆØ±ÙˆØ¯ Ø¨Ù‡: {category_url}")
        wait_for_internet()
        driver.get(category_url)

        # ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø®ØªØ§Ø± ØµÙØ­Ù‡ ØªÙˆØ³Ø· AI
        page_analysis = ai_optimizer.analyze_page_structure(driver, "list")
        strategy = page_analysis["strategy"]
        log(f"Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡: {strategy['type']} (Ø§Ø¹ØªÙ…Ø§Ø¯: {page_analysis['confidence_score']:.2f})")

        close_map_if_exists(driver)

        seen_ordered: List[str] = []
        seen_set: Set[str] = set()
        last_unique_count = 0
        no_new_rounds = 0
        max_rounds = SCROLL_MAX_ROUNDS

        if strategy["type"] == "infinite_scroll":
            max_rounds = strategy.get("max_attempts", SCROLL_MAX_ROUNDS)

        for round_idx in range(1, max_rounds + 1):
            anchors = driver.find_elements(By.CSS_SELECTOR,
                                           "article.kt-post-card a[href], a.kt-post-card__action[href], article a[href]")
            dom_cards = driver.find_elements(By.CSS_SELECTOR, "article.kt-post-card")
            dom_count = len(dom_cards)

            for a in anchors:
                try:
                    href = a.get_attribute("href") or ""
                except Exception:
                    href = ""
                href = href.strip()
                if not href:
                    continue
                if href.startswith("/"):
                    href = "https://divar.ir" + href
                if "/v/" not in href:
                    continue
                if href not in seen_set:
                    seen_set.add(href)
                    seen_ordered.append(href)

            log(f"[round {round_idx}] DOM_cards={dom_count} | unique_links={len(seen_ordered)}")

            # Ø§Ø¹Ù…Ø§Ù„ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„ AI
            try:
                if strategy["type"] == "infinite_scroll":
                    scroll_amount = strategy.get("scroll_increment", 800)
                    driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
                    human_sleep(*strategy.get("scroll_delay", LIST_SCROLL_SLEEP))
                elif strategy["type"] == "standard_scroll":
                    if dom_cards:
                        driver.execute_script("arguments[0].scrollIntoView({block: 'end'});", dom_cards[-1])
                    else:
                        scroll_amount = strategy.get("scroll_increment", 600)
                        driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
                    human_sleep(*strategy.get("scroll_delay", LIST_SCROLL_SLEEP))
            except Exception:
                driver.execute_script("window.scrollBy(0, window.innerHeight);")
                human_sleep(*LIST_SCROLL_SLEEP)

            if len(seen_ordered) == last_unique_count:
                no_new_rounds += 1
            else:
                no_new_rounds = 0
                last_unique_count = len(seen_ordered)

            if no_new_rounds >= SCROLL_PATIENCE:
                log(f"ØªÙˆÙ‚Ù: {no_new_rounds} Ø¯ÙˆØ± Ù¾ÛŒØ§Ù¾ÛŒ Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ù†ÛŒØ§Ù…Ø¯ (patience={SCROLL_PATIENCE}).")
                for _ in range(SCROLL_EXTRA_AFTER_STABLE):
                    driver.execute_script("window.scrollBy(0, 2000);")
                    human_sleep(*LIST_SCROLL_SLEEP)
                break

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‡Ø§ÛŒÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        human_sleep(0.9, 1.3)
        anchors = driver.find_elements(By.CSS_SELECTOR,
                                       "article.kt-post-card a[href], a.kt-post-card__action[href], article a[href]")
        for a in anchors:
            try:
                href = a.get_attribute("href") or ""
            except:
                href = ""
            href = href.strip()
            if not href:
                continue
            if href.startswith("/"):
                href = "https://divar.ir" + href
            if "/v/" not in href:
                continue
            if href not in seen_set:
                seen_set.add(href)
                seen_ordered.append(href)

        log(f"ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ: {len(seen_ordered)}")

        # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬
        success_rate = min(len(seen_ordered) / 50, 1.0)  # Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª ØªÙ‚Ø±ÛŒØ¨ÛŒ
        ai_optimizer.learn_from_results(category_url, strategy, success_rate, {"links_count": len(seen_ordered)})

        return seen_ordered

    finally:
        try:
            driver.quit()
        except Exception:
            pass


def click_show_all_details(driver: webdriver.Chrome) -> bool:
    """
    ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ú©Ù„ÛŒÚ© Ø±ÙˆÛŒ Â«Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…Ù‡Ù” Ø¬Ø²Ø¦ÛŒØ§ØªÂ» - Ù†Ø³Ø®Ù‡ Ø¨Ø³ÛŒØ§Ø± Ø³Ø§Ø¯Ù‡
    """
    try:
        log("ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ú©Ù…Ù‡ 'Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…Ù‡Ù” Ø¬Ø²Ø¦ÛŒØ§Øª'...")

        # Ø§ÙˆÙ„ ØµÙØ­Ù‡ Ø±Ùˆ Ø®ÙˆØ¨ Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ù†ÛŒÙ…
        driver.execute_script("window.scrollBy(0, 800);")
        human_sleep(0.1, 0.9)
        driver.execute_script("window.scrollBy(0, 400);")
        human_sleep(0.1, 0.8)

        # ğŸ’¡ Ø±ÙˆØ´ 1: Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ´ - Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…
        try:
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø§Ù„Ù…Ø§Ù† Ø¨Ø§ Ù…ØªÙ† Ø¯Ù‚ÛŒÙ‚
            show_more_element = driver.find_element(By.XPATH, "//*[text()='Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…Ù‡Ù” Ø¬Ø²Ø¦ÛŒØ§Øª']")
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", show_more_element)
            human_sleep(0.1, 1)
            driver.execute_script("arguments[0].click();", show_more_element)
            log("âœ… Ú©Ù„ÛŒÚ© Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨Ø§ Ù…ØªÙ† Ø¯Ù‚ÛŒÙ‚")
            human_sleep(0.4, 1.1)
            return True
        except:
            pass

        # ğŸ’¡ Ø±ÙˆØ´ 2: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø§ contains
        try:
            show_more_elements = driver.find_elements(By.XPATH, "//*[contains(text(), 'Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…Ù‡')]")
            for element in show_more_elements:
                try:
                    if element.is_displayed():
                        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", element)
                        human_sleep(0.5, 1)
                        driver.execute_script("arguments[0].click();", element)
                        log("âœ… Ú©Ù„ÛŒÚ© Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨Ø§ contains")
                        human_sleep(1.0, 2.0)
                        return True
                except:
                    continue
        except:
            pass

        # ğŸ’¡ Ø±ÙˆØ´ 3: Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ JavaScript
        try:
            result = driver.execute_script("""
                // Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§
                var allElements = document.querySelectorAll('*');
                for (var i = 0; i < allElements.length; i++) {
                    var element = allElements[i];
                    var text = element.textContent || element.innerText || '';

                    // Ø§Ú¯Ø± Ù…ØªÙ† Ø´Ø§Ù…Ù„ 'Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…Ù‡' Ø¨Ø§Ø´Ø¯
                    if (text.includes('Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…Ù‡Ù” Ø¬Ø²Ø¦ÛŒØ§Øª') || text.includes('Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…Ù‡')) {
                        // Ø§Ø³Ú©Ø±ÙˆÙ„ Ùˆ Ú©Ù„ÛŒÚ©
                        element.scrollIntoView({behavior: 'smooth', block: 'center'});
                        element.click();
                        return true;
                    }
                }
                return false;
            """)

            if result:
                log("âœ… Ú©Ù„ÛŒÚ© Ø¨Ø§ JavaScript Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯")
                human_sleep(1.0, 2.0)
                return True
        except Exception as js_error:
            log(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± JavaScript: {js_error}")

        log("âš ï¸ Ø¯Ú©Ù…Ù‡ 'Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª' Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ù…Ù…Ú©Ù† Ø§Ø³Øª ØµÙØ­Ù‡ Ø§Ø² Ù‚Ø¨Ù„ Ú¯Ø³ØªØ±Ø´ ÛŒØ§ÙØªÙ‡ Ø¨Ø§Ø´Ø¯.")
        return False

    except Exception as e:
        log(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ú©Ù„ÛŒÚ© Ù†Ù…Ø§ÛŒØ´ Ø¬Ø²Ø¦ÛŒØ§Øª: {e}")
        return False


def extract_value_by_title(soup: BeautifulSoup, title_text: str, default: str = "Ù†Ø§Ù…Ø´Ø®Øµ") -> str:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù†ÙˆØ§Ù†
    """
    try:
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§ Ù…ØªÙ† Ø¹Ù†ÙˆØ§Ù†
        title_elements = soup.find_all(["p", "span", "div"], string=re.compile(f".*{re.escape(title_text)}.*"))

        for title_el in title_elements:
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù‡Ù… level Ø¯Ø± Ú©Ù†Ø§Ø± Ø¹Ù†ÙˆØ§Ù†
            parent = title_el.find_parent()
            if parent:
                # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø± Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§ÙˆØ±
                value_elements = parent.find_all(["p", "span", "div"],
                                                 class_=re.compile("value|end|value-box|amount|number"))

                for value_el in value_elements:
                    if value_el != title_el and value_el.get_text(strip=True):
                        raw_value = value_el.get_text(strip=True)

                        # Ø§Ú¯Ø± ÙÛŒÙ„Ø¯ Ø¹Ø¯Ø¯ÛŒ Ù‡Ø³ØªØŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ú©Ù†
                        if title_text in ['Ù‚ÛŒÙ…Øª Ú©Ù„', 'Ù‚ÛŒÙ…Øª Ù‡Ø± Ù…ØªØ±', 'Ø·Ø¨Ù‚Ù‡']:
                            cleaned_value = re.sub(r'[^\d]', '', raw_value)
                            return cleaned_value if cleaned_value else "Ù†Ø§Ù…Ø´Ø®Øµ"
                        else:
                            return raw_value

                # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± sibling elements
                next_sibling = title_el.find_next_sibling()
                if next_sibling and next_sibling.get_text(strip=True):
                    raw_value = next_sibling.get_text(strip=True)
                    if title_text in ['Ù‚ÛŒÙ…Øª Ú©Ù„', 'Ù‚ÛŒÙ…Øª Ù‡Ø± Ù…ØªØ±', 'Ø·Ø¨Ù‚Ù‡']:
                        cleaned_value = re.sub(r'[^\d]', '', raw_value)
                        return cleaned_value if cleaned_value else "Ù†Ø§Ù…Ø´Ø®Øµ"
                    else:
                        return raw_value

        return default

    except Exception:
        return default


def scrape_ad_detail(driver: webdriver.Chrome, link: str, category: str) -> Optional[Dict[str, str]]:
    """
    Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ Ø¢Ú¯Ù‡ÛŒØŒ Ú©Ù„ÛŒÚ© Ù†Ù…Ø§ÛŒØ´ Ø¬Ø²ÛŒÛŒØ§ØªØŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª Ùˆ Ø§Ù…Ú©Ø§Ù†Ø§Øª
    """
    try:
        wait_for_internet()
        driver.get(link)
        human_sleep(*DETAIL_DWELL)

        # Ø¨Ø³ØªÙ† pop-up Ù‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
        try:
            popup_selectors = [
                "button[aria-label='Ø¨Ø³ØªÙ†']",
                "div[class*='close']",
                "button[class*='close']",
                "svg[class*='close']"
            ]
            for selector in popup_selectors:
                try:
                    close_btn = driver.find_element(By.CSS_SELECTOR, selector)
                    close_btn.click()
                    human_sleep(0.3, 0.7)
                except:
                    continue
        except:
            pass

        # ğŸ’¡ Ù…Ù‡Ù…: Ù‚Ø¨Ù„ Ø§Ø² Ú©Ù„ÛŒÚ© Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ù†ÛŒÙ…
        driver.execute_script("window.scrollBy(0, 500);")
        human_sleep(1.0, 1.5)

        # ğŸ’¡ Ø§ÙˆÙ„ ØµÙØ­Ù‡ Ø±Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†ÛŒÙ… (Ù‚Ø¨Ù„ Ø§Ø² Ú©Ù„ÛŒÚ©)
        soup_before_click = BeautifulSoup(driver.page_source, "html.parser")

        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±
        clicked = click_show_all_details(driver)

        if clicked:
            log("âœ… Ú©Ù„ÛŒÚ© Ù…ÙˆÙÙ‚ØŒ Ù…Ù†ØªØ¸Ø± Ù„ÙˆØ¯ Ø¬Ø²Ø¦ÛŒØ§Øª...")
            human_sleep(2.0, 3.0)  # Ø²Ù…Ø§Ù† Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ø¬Ø²Ø¦ÛŒØ§Øª

            # ğŸ’¡ Ø¨Ø¹Ø¯ Ø§Ø² Ú©Ù„ÛŒÚ©ØŒ ØµÙØ­Ù‡ Ø±Ùˆ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†ÛŒÙ…
            soup_after_click = BeautifulSoup(driver.page_source, "html.parser")

            # Ø§Ø² ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ú©Ù„ÛŒÚ© Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
            soup = soup_after_click
        else:
            log("âš ï¸ Ú©Ù„ÛŒÚ© Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ¹Ù„ÛŒ")
            human_sleep(1.0, 1.5)
            # Ø§Ø² ØµÙØ­Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ú©Ù„ÛŒÚ© Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
            soup = soup_before_click

        # Ø§Ø³Ú©Ø±ÙˆÙ„ Ù…Ø¬Ø¯Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
        driver.execute_script("window.scrollBy(0, 300);")
        human_sleep(0.8, 1.2)

        data: Dict[str, str] = {"category": category, "Ù„ÛŒÙ†Ú©": link}

        # Ø¹Ù†ÙˆØ§Ù†
        title_el = soup.select_one("h1.kt-page-title__title")
        data["Ø¹Ù†ÙˆØ§Ù†"] = title_el.get_text(" ", strip=True) if title_el else None

        # ØªØ§Ø±ÛŒØ®/Ù…Ú©Ø§Ù†
        sub = soup.select_one("div.kt-page-title__subtitle")
        if sub:
            txt = sub.get_text(" ", strip=True)
            m = re.match(r"(.+?)\s+Ø¯Ø±\s+(.+)", txt)
            if m:
                data["ØªØ§Ø±ÛŒØ®"] = m.group(1).strip() if m.group(1).strip() != "Ù†Ø§Ù…Ø´Ø®Øµ" else None
                data["Ù…Ú©Ø§Ù†"] = m.group(2).strip() if m.group(2).strip() != "Ù†Ø§Ù…Ø´Ø®Øµ" else None
            else:
                data["ØªØ§Ø±ÛŒØ®"] = None
                data["Ù…Ú©Ø§Ù†"] = txt if txt != "Ù†Ø§Ù…Ø´Ø®Øµ" else None
        else:
            data["ØªØ§Ø±ÛŒØ®"], data["Ù…Ú©Ø§Ù†"] = None, None

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®Ø§Øµ Ø§Ø² Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ú©Ù„Ø§Ø³ Ù…Ø´Ø®Øµ
        extract_specific_details(soup, data)

        # Ù…ØªØ±Ø§Ú˜/Ø³Ø§Ù„ Ø³Ø§Ø®Øª/ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚
        data["Ù…ØªØ±Ø§Ú˜"] = data["Ø³Ø§Ù„ Ø³Ø§Ø®Øª"] = data["ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚"] = None
        try:
            rows = soup.select("tr.kt-group-row__data-row")
            for row in rows:
                cells = row.select("td.kt-group-row-item--info-row, td.kt-group-row-item.kt-group-row-item__value")
                if not cells:
                    continue
                vals = [c.get_text(" ", strip=True) for c in cells]
                if len(vals) >= 3:
                    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ
                    meterage_clean = re.sub(r'[^\d]', '', vals[0])
                    year_clean = re.sub(r'[^\d]', '', vals[1])
                    rooms_clean = re.sub(r'[^\d]', '', vals[2])

                    data["Ù…ØªØ±Ø§Ú˜"] = int(meterage_clean) if meterage_clean else None
                    data["Ø³Ø§Ù„ Ø³Ø§Ø®Øª"] = int(year_clean) if year_clean else None
                    data["ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚"] = int(rooms_clean) if rooms_clean else None
                    break
                elif len(vals) == 2:
                    meterage_clean = re.sub(r'[^\d]', '', vals[0])
                    year_clean = re.sub(r'[^\d]', '', vals[1])
                    data["Ù…ØªØ±Ø§Ú˜"] = int(meterage_clean) if meterage_clean else None
                    data["Ø³Ø§Ù„ Ø³Ø§Ø®Øª"] = int(year_clean) if year_clean else None
                    break
                elif len(vals) == 1:
                    meterage_clean = re.sub(r'[^\d]', '', vals[0])
                    data["Ù…ØªØ±Ø§Ú˜"] = int(meterage_clean) if meterage_clean else None
                    break
        except Exception:
            pass

        # Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§
        price_total = extract_value_by_title(soup, "Ù‚ÛŒÙ…Øª Ú©Ù„", "Ù†Ø§Ù…Ø´Ø®Øµ")
        price_per_meter = extract_value_by_title(soup, "Ù‚ÛŒÙ…Øª Ù‡Ø± Ù…ØªØ±", "Ù†Ø§Ù…Ø´Ø®Øµ")
        floor = extract_value_by_title(soup, "Ø·Ø¨Ù‚Ù‡", "Ù†Ø§Ù…Ø´Ø®Øµ")

        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ
        data["Ù‚ÛŒÙ…Øª Ú©Ù„"] = int(re.sub(r'[^\d]', '', price_total)) if price_total != "Ù†Ø§Ù…Ø´Ø®Øµ" else None
        data["Ù‚ÛŒÙ…Øª Ù‡Ø± Ù…ØªØ±"] = int(re.sub(r'[^\d]', '', price_per_meter)) if price_per_meter != "Ù†Ø§Ù…Ø´Ø®Øµ" else None
        data["Ø·Ø¨Ù‚Ù‡"] = int(re.sub(r'[^\d]', '', floor)) if floor != "Ù†Ø§Ù…Ø´Ø®Øµ" else None

        # Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø±Ø´ØªÙ‡â€ŒØ§ÛŒ Ùˆ Ø³ØªÙˆÙ†ÛŒ
        feature_titles = [p.get_text(strip=True) for p in soup.find_all("p", class_="kt-feature-row__title")]
        data["ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø§Ù…Ú©Ø§Ù†Ø§Øª"] = ", ".join(feature_titles) if feature_titles else None

        # ØªÙˆØ¶ÛŒØ­Ø§Øª - Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        desc = soup.select_one("p.kt-description-row__text.kt-description-row__text--primary")
        if not desc:
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø±Ø§ÛŒ ØªÙˆØ¶ÛŒØ­Ø§Øª
            desc_selectors = [
                "p.kt-description-row__text",
                "div.kt-description-row__text",
                "div[class*='description']",
                "p[class*='description']"
            ]
            for selector in desc_selectors:
                desc = soup.select_one(selector)
                if desc:
                    break

        if desc:
            data["ØªÙˆØ¶ÛŒØ­Ø§Øª"] = "\n".join([ln.strip() for ln in desc.get_text("\n").splitlines() if ln.strip()])
        else:
            data["ØªÙˆØ¶ÛŒØ­Ø§Øª"] = None

        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        data = clean_numeric_fields(data)

        # Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ù‡Ù… Ø§Ú¯Ø± Ù†Ø§Ù…Ø´Ø®Øµ Ø¨ÙˆØ¯ØŒ null Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡
        text_fields = ['Ø¹Ù†ÙˆØ§Ù†', 'Ù…Ú©Ø§Ù†', 'ØªØ§Ø±ÛŒØ®', 'Ù†ÙˆØ¹ Ø³Ù†Ø¯', 'ÙˆØ¶Ø¹ÛŒØª ÙˆØ§Ø­Ø¯',
                       'Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†', 'Ø¬Ù†Ø³ Ú©Ù', 'Ù†ÙˆØ¹ Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ',
                       'Ù†ÙˆØ¹ Ø³Ø±Ù…Ø§ÛŒØ´', 'Ù†ÙˆØ¹ Ú¯Ø±Ù…Ø§ÛŒØ´', 'ØªØ§Ù…ÛŒÙ† Ú©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù…',
                       'ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø§Ù…Ú©Ø§Ù†Ø§Øª', 'ØªÙˆØ¶ÛŒØ­Ø§Øª']

        for field in text_fields:
            if field in data and data[field] in ['Ù†Ø§Ù…Ø´Ø®Øµ', '']:
                data[field] = None

        # Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø§Ù…Ú©Ø§Ù†Ø§Øª Ù‡Ù… null Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡ Ø§Ú¯Ø± Ù†Ø¯Ø§Ø±Ø¯ Ø¨Ø§Ø´Ù‡
        for feature_col in FEATURES_MAP.values():
            if feature_col in data and data[feature_col] == 'Ù†Ø¯Ø§Ø±Ø¯':
                data[feature_col] = None

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ® Ø§ÛŒØ¬Ø§Ø¯
        data["ØªØ§Ø±ÛŒØ® Ø§ÛŒØ¬Ø§Ø¯"] = get_current_timestamp()

        return data

    except Exception as e:
        log(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† Ø¬Ø²Ø¦ÛŒØ§Øª {link}: {e}")
        traceback.print_exc()
        return None


def save_to_excel(rows: List[Dict[str, str]], filename: str = OUTPUT_XLSX) -> None:
    if not rows:
        log("Ú†ÛŒØ²ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        return

    # ØªØ¨Ø¯ÛŒÙ„ Ù…Ù‚Ø§Ø¯ÛŒØ± 'Ù†Ø§Ù…Ø´Ø®Øµ' Ø¨Ù‡ None Ù‚Ø¨Ù„ Ø§Ø² Ø³Ø§Ø®Øª DataFrame
    for row in rows:
        for key, value in row.items():
            if value in ['Ù†Ø§Ù…Ø´Ø®Øµ', 'Ù†Ø¯Ø§Ø±Ø¯', '']:
                row[key] = None

    df_new = pd.DataFrame(rows)

    for col in FINAL_COLUMNS:
        if col not in df_new.columns:
            df_new[col] = None  # Ø¨Ù‡ Ø¬Ø§ÛŒ "Ù†Ø§Ù…Ø´Ø®Øµ" Ø§Ø² None Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    df_new = df_new[FINAL_COLUMNS]

    if os.path.exists(filename):
        try:
            df_old = pd.read_excel(filename)
            # Ù…Ø·Ù…Ø¦Ù† Ø´Ùˆ Ú©Ù‡ ÙØ§ÛŒÙ„ Ù‚Ø¯ÛŒÙ…ÛŒ Ù‡Ù… Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ø´Ø®Øµ Ø±Ùˆ Ø¨Ù‡ None ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†Ù‡
            for col in df_old.columns:
                df_old[col] = df_old[col].replace(['Ù†Ø§Ù…Ø´Ø®Øµ', 'Ù†Ø¯Ø§Ø±Ø¯', ''], None)
        except Exception:
            df_old = pd.DataFrame(columns=FINAL_COLUMNS)

        df_combined = pd.concat([df_old, df_new], ignore_index=True)
        if "Ù„ÛŒÙ†Ú©" in df_combined.columns:
            df_combined.drop_duplicates(subset=["Ù„ÛŒÙ†Ú©"], keep="last", inplace=True)
        df_combined.to_excel(filename, index=False)
    else:
        df_new.to_excel(filename, index=False)

    log(f"Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filename} (Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {len(pd.read_excel(filename))})")


def dedupe_links(all_links: List[str]) -> List[str]:
    seen_csv = read_seen_links_csv(SEEN_LINKS_CSV)
    seen_json = read_seen_links_json(SEEN_LINKS_JSON)
    seen_excel = load_existing_links_from_excel(OUTPUT_XLSX)
    seen = seen_csv | seen_json | seen_excel
    filtered = [lk for lk in all_links if lk not in seen]
    log(f"Ø¨Ø¹Ø¯ Ø§Ø² Ø­Ø°Ù Ø¯ÙˆÙ¾Ù„ÛŒÚ©ÛŒØªâ€ŒÙ‡Ø§: {len(filtered)} Ø§Ø² {len(all_links)}")
    return filtered


def ask_how_many(max_n: int) -> int:
    # Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆØ±ØŒ Ù‡Ù…Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÙˆÙ†Ø¯
    return max_n


# ----------------------------- Ø§ØµÙ„ÛŒ -----------------------------
def main():
    # Ø§Ø¨ØªØ¯Ø§ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…
    if not check_system_dependencies():
        log("âš ï¸ Ø¨Ø±Ø®ÛŒ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ ÛŒØ§ÙØª Ù†Ø´Ø¯Ù†Ø¯ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø§ Ø±ÛŒØ³Ú©...")

    log(f"Ø´Ø±ÙˆØ¹ Ø§Ø³Ú©Ø±Ù¾ Ù‡ÙˆØ´Ù…Ù†Ø¯: {CATEGORY_NAME} â€” {CATEGORY_URL}")

    # ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¯Ø±Ø§ÛŒÙˆØ± Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹ Ø§ØµÙ„ÛŒ
    try:
        log("ğŸ§ª ØªØ³Øª Ø§ÙˆÙ„ÛŒÙ‡ Ø§ØªØµØ§Ù„ Ø¯Ø±Ø§ÛŒÙˆØ±...")
        test_driver = build_driver(headless=True)
        test_driver.quit()
        log("âœ… ØªØ³Øª Ø§ØªØµØ§Ù„ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯")
    except Exception as e:
        log(f"âŒ ØªØ³Øª Ø§ØªØµØ§Ù„ Ù†Ø§Ù…ÙˆÙÙ‚: {e}")
        log("ğŸ”¥ Ø§Ø¯Ø§Ù…Ù‡ Ø¹Ù…Ù„ÛŒØ§Øª Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø§ Ù…Ø´Ú©Ù„ Ù…ÙˆØ§Ø¬Ù‡ Ø´ÙˆØ¯")

    # Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² AI
    ai_optimizer = AIScrapingOptimizer()

    # Ø§Ú¯Ø± checkpoint ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù‡ØŒ Ø§Ø² Ù‡Ù…ÙˆÙ† Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒØ¯ÛŒÙ…
    checkpoint = load_checkpoint(CHECKPOINT_FILE)
    if checkpoint:
        log("ğŸ” checkpoint Ù¾ÛŒØ¯Ø§ Ø´Ø¯ â€” Ø§Ø¯Ø§Ù…Ù‡ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡.")
        # Ø³Ø§Ø®ØªØ§Ø± checkpoint Ù…Ø§: { "to_process": [...], "next_idx": int, "processed_links": [...], "scraped_rows": [...] }
        to_process = checkpoint.get("to_process", [])
        next_idx = checkpoint.get("next_idx", 1)
        scraped_rows = checkpoint.get("scraped_rows", [])
        processed_links = checkpoint.get("processed_links", [])
        # Ø§Ú¯Ø± to_process Ø®Ø§Ù„ÛŒÙ‡ØŒ Ù…Ù…Ú©Ù†Ù‡ Ù†ÛŒØ§Ø² Ø¨Ø§Ø´Ù‡ Ù„ÛŒØ³Øª Ø¬Ø¯ÛŒØ¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø±Ùˆ Ø¨Ú¯ÛŒØ±ÛŒÙ….
        if not to_process:
            log("âš ï¸ Ù„ÛŒØ³Øª to_process Ø¯Ø± checkpoint Ø®Ø§Ù„ÛŒ Ø§Ø³Øª â€” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

            # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯Ø±Ø§ÛŒÙˆØ± Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
            try:
                all_links = get_ad_links_ai(CATEGORY_URL, CATEGORY_NAME, ai_optimizer)
            except Exception as e:
                log(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: {e}")
                return

            if not all_links:
                log("Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú©ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
                return
            new_links = dedupe_links(all_links)
            if not new_links:
                log("ØªÙ…Ø§Ù… Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² Ù‚Ø¨Ù„ Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.")
                return
            n = ask_how_many(len(new_links))
            to_process = new_links[:n]
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ checkpoint Ø§ÙˆÙ„ÛŒÙ‡
            checkpoint_state = {
                "to_process": to_process,
                "next_idx": 1,
                "processed_links": processed_links,
                "scraped_rows": scraped_rows
            }
            save_checkpoint(CHECKPOINT_FILE, checkpoint_state)
    else:
        # Ø­Ø§Ù„Øª Ø¹Ø§Ø¯ÛŒ: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ ØªÙˆØ³Ø· AI
        try:
            all_links = get_ad_links_ai(CATEGORY_URL, CATEGORY_NAME, ai_optimizer)
        except Exception as e:
            log(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: {e}")
            return

        if not all_links:
            log("Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú©ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
            return

        # Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØ¯Ù‡â€ŒØ´Ø¯Ù‡
        new_links = dedupe_links(all_links)
        if not new_links:
            log("ØªÙ…Ø§Ù… Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² Ù‚Ø¨Ù„ Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.")
            return

        # Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆØ±ØŒ Ù‡Ù…Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÙˆÙ†Ø¯
        n = ask_how_many(len(new_links))
        to_process = new_links[:n]
        log(f"{len(to_process)} Ù„ÛŒÙ†Ú© Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯.")

        # Ø§ÛŒØ¬Ø§Ø¯ checkpoint Ø§ÙˆÙ„ÛŒÙ‡
        scraped_rows = []
        processed_links = []
        next_idx = 1
        checkpoint_state = {
            "to_process": to_process,
            "next_idx": next_idx,
            "processed_links": processed_links,
            "scraped_rows": scraped_rows
        }
        save_checkpoint(CHECKPOINT_FILE, checkpoint_state)

    # Ø¯Ø±Ø§ÛŒÙˆØ± Ø¯ÙˆÙ… Ø¨Ø±Ø§ÛŒ Ø¬Ø²Ø¦ÛŒØ§Øª - Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
    try:
        detail_driver = build_driver(headless=True)  # headless=True Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆØ±
        log("âœ… Ø¯Ø±Ø§ÛŒÙˆØ± Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    except Exception as e:
        log(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯Ø±Ø§ÛŒÙˆØ± Ø¬Ø²Ø¦ÛŒØ§Øª: {e}")
        log("ğŸ”¥ Ø§Ø¯Ø§Ù…Ù‡ Ø¹Ù…Ù„ÛŒØ§Øª ØºÛŒØ±Ù…Ù…Ú©Ù† Ø§Ø³Øª")
        return

    success_count = 0

    try:
        total = len(to_process)
        log(f"Ø¢ØºØ§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ {total} Ù„ÛŒÙ†Ú© (Ø´Ø±ÙˆØ¹ Ø§Ø² idx={next_idx})")

        # iterator Ø§Ø² idx Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡
        for idx in range(next_idx, total + 1):
            link = to_process[idx - 1]
            log(f"[{idx}/{total}] Ù¾Ø±Ø¯Ø§Ø²Ø´: {link}")

            try:
                # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø¯Ø±Ø§ÛŒÙˆØ± Ù‚Ø¨Ù„ Ø§Ø² Ù‡Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´
                try:
                    detail_driver.current_url  # ØªØ³Øª Ø³Ø§Ø¯Ù‡ Ø§ØªØµØ§Ù„
                except Exception:
                    log("âš ï¸ Ø¯Ø±Ø§ÛŒÙˆØ± Ù‚Ø·Ø¹ Ø´Ø¯Ù‡ØŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯...")
                    try:
                        detail_driver.quit()
                    except:
                        pass
                    detail_driver = build_driver(headless=True)
                    log("âœ… Ø¯Ø±Ø§ÛŒÙˆØ± Ù…Ø¬Ø¯Ø¯Ø§Ù‹ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")

                row = scrape_ad_detail(detail_driver, link, CATEGORY_NAME)
                if row:
                    scraped_rows.append(row)
                    processed_links.append(link)
                    success_count += 1

                    # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ù…ÙˆÙÙ‚
                    ai_optimizer.learn_from_results(
                        link,
                        {"type": "detail_extraction"},
                        1.0,  # Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª
                        row
                    )
                else:
                    # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§Ù‡Ø§
                    ai_optimizer.learn_from_results(
                        link,
                        {"type": "detail_extraction"},
                        0.0,  # Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª
                        {}
                    )
                    log("Ø±Ø¯ Ø´Ø¯ ÛŒØ§ Ø®Ø·Ø§ Ø¯Ø§Ø´Øª.")

            except Exception as e:
                # Ø§Ú¯Ø± Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡â€ŒØ§ÛŒ ÙˆØ³Ø· Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ´ Ø§ÙˆÙ…Ø¯ØŒ Ù„Ø§Ú¯ Ú©Ù† Ùˆ Ø°Ø®ÛŒØ±Ù‡ checkpoint Ø³Ù¾Ø³ Ø§Ø¯Ø§Ù…Ù‡ ÛŒØ§ Ø®Ø§Ø±Ø¬ Ø´Ùˆ
                log(f"âš ï¸ Ø®Ø·Ø§ Ù‡Ù†Ú¯Ø§Ù… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù„ÛŒÙ†Ú© {link}: {e}")
                traceback.print_exc()

            # Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± Ø¢Ú¯Ù‡ÛŒØŒ checkpoint Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒØ´Ù‡ (Ø§ØªÙˆ Ø³ÛŒÙˆ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ)
            next_idx = idx + 1
            checkpoint_state = {
                "to_process": to_process,
                "next_idx": next_idx,
                "processed_links": processed_links,
                "scraped_rows": scraped_rows
            }
            save_checkpoint(CHECKPOINT_FILE, checkpoint_state)

            human_sleep(*BETWEEN_ADS_SLEEP)

    except Exception as e:
        log(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø­ÛŒÙ† Ù¾Ø±Ø¯Ø§Ø²Ø´: {e}")
        traceback.print_exc()
    finally:
        try:
            detail_driver.quit()
            log("âœ… Ø¯Ø±Ø§ÛŒÙˆØ± Ø¨Ø³ØªÙ‡ Ø´Ø¯")
        except Exception as e:
            log(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø³ØªÙ† Ø¯Ø±Ø§ÛŒÙˆØ±: {e}")

    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ (Ø§Ú¯Ø± Ú†ÛŒØ²ÛŒ Ø¬Ù…Ø¹ Ø´Ø¯Ù‡)
    if scraped_rows:
        try:
            save_to_excel(scraped_rows, OUTPUT_XLSX)
            append_seen_links_csv(SEEN_LINKS_CSV, processed_links)
            write_seen_links_json(SEEN_LINKS_JSON, set(list(read_seen_links_json(SEEN_LINKS_JSON)) + processed_links))
            log(f"{len(processed_links)} Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")

            # Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ AI
            success_rate = success_count / len(to_process) if to_process else 0.0
            log(f"Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø³ØªØ®Ø±Ø§Ø¬: {success_rate:.2%}")

            # Ù¾Ø³ Ø§Ø² Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‡Ø§ÛŒÛŒØŒ checkpoint Ù¾Ø§Ú© Ù…ÛŒØ´Ù‡ ØªØ§ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø´Ø±ÙˆØ¹ Ú©Ù†Ù‡
            clear_checkpoint(CHECKPOINT_FILE)
            log("âœ… checkpoint Ù¾Ø§Ú© Ø´Ø¯")

        except Exception as e:
            log(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ: {e}")
            log("âš ï¸ checkpoint Ø­ÙØ¸ Ø´Ø¯ ØªØ§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² Ø¯Ø³Øª Ù†Ø±ÙˆÙ†Ø¯")

    else:
        log("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†Ø¨ÙˆØ¯. checkpoint Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø¯ÙØ¹Ù‡ Ø¨Ø¹Ø¯ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ù‡ÛŒØ¯.")

    log("Ù¾Ø§ÛŒØ§Ù† Ø§Ø³Ú©Ø±Ù¾ Ù‡ÙˆØ´Ù…Ù†Ø¯.")

if __name__ == "__main__":
        main()
